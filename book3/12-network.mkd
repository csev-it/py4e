%

Programmi per Internet
=====================



Molti degli esempi in questo libro si sono concentrati sulla lettura di file e sulla ricerca di dati al loro interno, ma in rete abbiamo molte altre fonti di informazione da cui attingere.
In questo capitolo faremo finta di essere un browser Web e recupereremo le pagine Web utilizzando l'HyperText Transfer Protocol (HTTP). Quindi leggeremo ed analizzeremo i dati presenti in esse contenuti.

HyperText Transfer Protocol - HTTP
-----------------------------------

Il protocollo di rete su cui si basa il web è in realtà piuttosto semplice ed il supporto integrato di Python chiamato `sockets` rende molto facile creare connessioni di rete e ottenere i dati su questi socket.

Un *socket* è molto simile a un file, tranne per il fatto che un singolo socket prevede una connessione a due vie tra due programmi. Il dato scritto in un socket viene inviato all'altro programma collegato.

Se avvii un processo di scrittura nel socket, otterrai i dati inviati dall'altra applicazione.

Se provi a leggere un socket senza che il programma all'altra estremità abbia inviato alcun dato, potrai solo sederti e aspettare. Se entrambi i programmi collegati alle estremità di un socket semplicemente aspettano dati senza inviare nulla, probabilmente attenderanno per molto tempo.

Quindi una componente importante dei programmi che comunicano in Internet è l'avere un qualche tipo di protocollo. Un protocollo è un insieme di regole precise che determinano chi può spedire dati per primo, cosa fanno, e poi quali sono le risposte a un messaggio, chi invia il prossimo, e così via. In un certo senso le due applicazioni alle due estremità del socket stanno ballando insieme, facendo attenzione a non pestarsi i piedi l'un l'altro.

C'è un'ampia letteratura che descrive questi protocolli di rete. Ad esempio l'HyperText Transfer Protocol è descritto nel seguente:

<http://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Avrai modo di vedere che è un documento lungo e complesso di 176 pagine contenente molti dettagli. Se lo trovi interessante, sentiti libero di leggerlo tutto. Per ora, dai un'occhiata a pagina 36 di RFC2616 dove troverai la sintassi per la richiesta GET. Proviamo a richiedere un documento da un server web: colleghiamoci al server `www.pr4e.org` sulla porta 80 e inviamo la seguente richiesta

`GET http://data.pr4e.org/romeo.txt HTTP / 1.0 `

il cui secondo parametro indica la pagina web che stiamo richiedendo, inviamo poi anche una riga vuota. Il server web risponderà con alcune informazioni di intestazione riguardanti il documento, una riga vuota e il contenuto del documento richiesto.

Il browser Web più semplice del mondo
--------------------------------

Forse il modo più facile per mostrare come funziona il protocollo HTTP è scrivere un semplice script Python che effettui una connessione a un server web e che, seguendo le regole del protocollo HTTP, richieda un documento e visualizzi ciò che il server ci restituisce.

\VerbatimInput{../code3/socket1.py}

Per prima cosa il programma effettua una connessione alla porta 80 del server [www.py4e.com] (http://www.py4e.com). Dato che il nostro programma sta impersonando un "browser web", il protocollo HTTP prevede che il comando GET sia seguito da una riga vuota.

![A Socket Connection](height=2.0in@../images/socket)

Una volta inviata la riga vuota, scriviamo un ciclo che riceva dal socket dati in blocchi da 512 caratteri e li visualizzi fino a quando non ci sono più dati da leggere (cioè quando recv() restituisce una stringa vuota).

Il programma produrrà l'output seguente:

~~~~
HTTP/1.1 200 OK
Date: Sun, 14 Mar 2010 23:52:41 GMT
Server: Apache
Last-Modified: Tue, 29 Dec 2009 01:31:22 GMT
ETag: "143c1b33-a7-4b395bea"
Accept-Ranges: bytes
Content-Length: 167
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~


L'output inizia con l'invio dell'intestazione dal server Web che descrive il documento. Ad esempio, l'intestazione `Content-Type` indica che il documento è un documento di testo (`text/plain`).

Dopo che il server ci ha fornito l'intestazione, viene mandata una riga vuota che indica la fine delle intestazioni e viene iniziato l'invio del file `romeo.txt`.

Questo esempio è servito per mostrarti come realizzare una connessione di rete di basso livello con un socket. I socket possono essere utilizzati per comunicare con un server Web, con un server di posta o con molti altri tipi di server. Tutto ciò che serve è trovare il documento che descrive il protocollo da utilizzare e scrivere il codice per inviare e ricevere i dati rispettando quanto indicato.

Dal momento che il protocollo più comunemente utilizzato è l'HTTP, Python è dotato di una libreria appositamente progettata per supportare il recupero di documenti e dati dal web.

Recupero di un'immagine tramite HTTP
-----------------------------

\index{urllib!image}
\index{image!jpg}
\index{jpg}

Nell'esempio precedente abbiamo recuperato un semplice file di testo che conteneva dei "ritorni a capo" e abbiamo semplicemente visualizzato i dati sullo schermo durante la sua esecuzione. Possiamo usare un programma simile per recuperare un'immagine attraverso l'uso di HTTP. Invece di mostrare i dati sullo schermo mentre il programma viene eseguito, stavolta immagazziniamo i dati in una stringa, eliminiamo le intestazioni ed infine salviamo i dati dell'immagine in un file:

\VerbatimInput{../code3/urljpeg.py}

Quando esegui il programma, otterrai il seguente output:

~~~~
$ python urljpeg.py
2920 2920
1460 4380
1460 5840
1460 7300
...
1460 62780
1460 64240
2920 67160
1460 68620
1681 70301
Header length 240
HTTP/1.1 200 OK
Date: Sat, 02 Nov 2013 02:15:07 GMT
Server: Apache
Last-Modified: Sat, 02 Nov 2013 02:01:26 GMT
ETag: "19c141-111a9-4ea280f8354b8"
Accept-Ranges: bytes
Content-Length: 70057
Connection: close
Content-Type: image/jpeg
~~~~

Noterai che l'intestazione di questo url, `Content-Type`, indica che il corpo del documento è un'immagine (`image/jpeg`). Al termine del programma, è possibile visualizzare l'immagine aprendo il file `stuff.jpg` con un visualizzatore di immagini.

Mentre il programma è in esecuzione potrai notare che non riceviamo solo 5120 caratteri ad ogni chiamata del metodo `recv()`: in realtà riceviamo il numero di caratteri trasferiti attraverso la rete dal server Web dalla chiamata di `recv()`. In questo esempio, abbiamo ottenuto 1460 o 2920 caratteri ogni volta che abbiamo richiesto fino a 5120 caratteri.

I risultati potrebbero variare a seconda della velocità della tua connessione. Ti prego di notare inoltre che nell'ultima chiamata a `recv()` otteniamo 1681 byte, la fine del flusso di dati, e nella successiva chiamata a `recv()` riceviamo una stringa di lunghezza zero che ci avvisa che il server ha chiamato `close()` al socket e non vi sono più dati in arrivo.

\index{time}
\index{time.sleep}

Possiamo rallentare le nostre successive chiamate `recv()` rimuovendo il segno di commento alla chiamata `time.sleep()`. In questo modo, possiamo ritardare di un quarto di secondo ogni chiamata per permettere al server di "anticiparci" e inviarci più dati prima di richiamare nuovamente `recv()`. Il programma ora si comporterà nel modo seguente:

~~~~
$ python urljpeg.py
1460 1460
5120 6580
5120 11700
...
5120 62900
5120 68020
2281 70301
Header length 240
HTTP/1.1 200 OK
Date: Sat, 02 Nov 2013 02:22:04 GMT
Server: Apache
Last-Modified: Sat, 02 Nov 2013 02:01:26 GMT
ETag: "19c141-111a9-4ea280f8354b8"
Accept-Ranges: bytes
Content-Length: 70057
Connection: close
Content-Type: image/jpeg
~~~~

Ora, tranne che nella prima e l'ultima chiamata a `recv()`, abbiamo ricevuto 5120 caratteri ad ogni richiesta di nuovi dati.

Tra le richieste `send()` inviate dal server e le richieste `recv()` provenienti dalla nostra applicazione c'è un buffer. Se il programma è con il delay attivo, il server potrebbe riempire il buffer del socket ed essere costretto a sospendere l'invio di ulteriori dati fino a quando il nostro script inizia a svuotare il buffer. La sospensione di invio o ricezione di dati viene chiamato "controllo di flusso".

\index{flow control}

Recupero di pagine Web con `urllib`
---------------------------------------------

Anche se possiamo inviare e ricevere manualmente dati via HTTP utilizzando la libreria socket, esiste un modo molto più semplice per eseguire questa attività in Python appoggiandosi alla libreria `urllib`.
`urllib` ti permette di trattare una pagina web come se fosse un file. Basta indicare quale pagina web si desidera recuperare e `urllib` gestisce tutti i dettagli del protocollo HTTP e dell'intestazione.

Il codice per leggere il file `romeo.txt` dal web usando `urllib` è il seguente:

\VerbatimInput{../code3/urllib1.py}

Una volta che la pagina web è stata aperta con `urllib.urlopen`, possiamo trattarla come un file e leggerne il contenuto usando un ciclo `for`.

Quando il programma è in esecuzione, vediamo solo il contenuto del file. Le intestazioni che sono state inviate vengono rimosse dal `urllib` e vengono restituiti solo i dati.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Ad esempio, possiamo scrivere un programma per recuperare `romeo.txt` e calcolare la frequenza di ogni parola presente nel file come rappresentato nel seguente script:

\VerbatimInput{../code3/urlwords.py}

Ancora una volta tengo a ripeterti che dopo aver avuto accesso alla pagina Web, possiamo leggerla come se si trattasse di un file locale.

Analisi dell'HTML e raccolta dati dal Web
---------------------------------

\index{web!scraping}
\index{parsing HTML}

Uno degli usi più comuni delle funzionalità di `urllib` è lo *scraping* dei contenuti presenti nel web. Con la parola *scraping* del Web ci riferiamo allo scrivere un programma che, fingendo di essere un browser Web, recuperi delle pagine, ne esamini i dati contenuti alla ricerca di precisi pattern.

Ad esempio, un motore di ricerca come Google esaminerà l'origine di una pagina Web, ne estrarrà i collegamenti ad altre pagine, le recupererà e ne estrarrà altri collegamenti e così via. Usando questa tecnica, gli *spider* di Google riescono a farsi strada attraverso quasi tutte le pagine sul web.

Google utilizza anche la frequenza dei link che trova in una determinata pagina per stabilire quanto sia "importante" quella pagina e quanto in alto debba apparire nei risultati di ricerca.

Analisi dell'HTML utilizzando le espressioni regolari
--------------------------------------

Un modo semplice per analizzare l'HTML consiste nell'utilizzare le espressioni regolari per cercare ed estrarre ripetutamente sottostringhe che corrispondono a un particolare pattern.

Ecco una semplice pagina web:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

Possiamo progettare un'espressione regolare per confrontare ed estrarre i valori del collegamento dal testo precedente come segue:

~~~~
href="http://.+?"
~~~~

La nostra espressione regolare va alla ricerca di stringhe che iniziano con "href="http://", seguito da uno o più caratteri (".+?") e da altre doppie virgolette. Il punto interrogativo aggiunto al ".+?" indica che il confronto deve essere fatto in un modo "non avido". Un confronto non avido cerca la stringa corrispondente *più piccola* possibile mentre un confronto avido cerca la stringa corrispondente *più grande* possibile.

\index{greedy}
\index{non-greedy}

Le parentesi aggiunte alla nostra espressione regolare indicano quale parte della stringa vorremmo estrarre.

Potremmo scrivere il seguente script:

\index{regex!parentheses}
\index{parentheses!regular expression}

\VerbatimInput{../code3/urlregex.py}

Il metodo `findall` fornisce un elenco di tutte le stringhe che corrispondono alla nostra espressione regolare, restituendoci solo il testo del link compreso tra le doppie virgolette.

Quando eseguiamo il programma, otteniamo il seguente risultato:

~~~~
python urlregex.py
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm
~~~~

~~~~
python urlregex.py
Enter - http://www.py4e.com/book.htm
http://www.greenteapress.com/thinkpython/thinkpython.html
http://allendowney.com/
http://www.py4e.com/code
http://www.lib.umich.edu/espresso-book-machine
http://www.py4e.com/py4inf-slides.zip
~~~~

Le espressioni regolari funzionano molto bene quando il codice HTML è ben formattato e prevedibile. Ma dal momento che ci sono molte pagine HTML "danneggiate", una soluzione che utilizzi solo espressioni regolari potrebbe non riportare alcuni collegamenti validi o fornire dati non validi.

Questo problema può essere risolto utilizzando una robusta libreria per analisi HTML.

Analisi dell'HTML con BeautifulSoup
--------------------------------

\index{BeautifulSoup}
Esistono numerose librerie Python che possono aiutarti ad analizzare pagine HTML ed estrarne i dati. Dal momento che ciascuna di queste librerie ha punti di forza e di debolezza, dovrai sceglierle in base alle tue esigenze.

Nelle pagine seguenti eseguiremo delle semplici analisi su del codice HTML e ne estrarremo i collegamenti usando la libreria *BeautifulSoup*. Puoi scaricare la libreria BeautifulSoup da:

<http://www.crummy.com/software/>

Hai la possibilità di "installare" BeautifulSoup^ [Puoi utilizzare il comando "pip install beautifulsoup".] o semplicemente salvare il file `BeautifulSoup.py` nella stessa cartella dell'applicazione.

Anche se l'HTML ha molto in comune con il formato XML^ [Il formato XML verrà affrontato nel prossimo capitolo.] e alcune pagine sono scritte appositamente per essere codice XML, la maggior parte del codice HTML è scritto in modi che fanno sì che un parser XML rigetti l'intera pagina di HTML in quanto formattata in modo improprio. BeautifulSoup tollera codice HTML scritto molto male consentendoti comunque di estrarre i dati di cui hai bisogno.

Useremo `urllib` per leggere una pagina e poi useremo `BeautifulSoup` per estrarre gli attributi `href` dagli "anchor tags" (`a`).

\index{BeautifulSoup}
\index{HTML}
\index{parsing!HTML}

\VerbatimInput{../code3/urllinks.py}

Il programma ti richiede un indirizzo Web, quindi:
- ne apre la pagina Web,
- ne legge i dati e li passa al parser BeautifulSoup,
- recupera tutti gli anchor tags,
- visualizza l'attributo `href` per ciascun tag.

Quando il programma viene eseguito visualizza quanto segue:

~~~~
python urllinks.py
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm
~~~~

~~~~
python urllinks.py
Enter - http://www.py4e.com/book.htm
http://www.greenteapress.com/thinkpython/thinkpython.html
http://allendowney.com/
http://www.si502.com/
http://www.lib.umich.edu/espresso-book-machine
http://www.py4e.com/code
http://www.py4e.com/
~~~~

Puoi utilizzare BeautifulSoup per estrarre varie parti di ciascun tag come indicato nelle pagine seguenti:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

Questi esempi iniziano solo a mostrarti la potenza di BeautifulSoup nell'analizzare del codice HTML.

Leggere file binari tramite urllib
---------------------------------

capiterà che tu voglia scaricare un file non di testo (o binario) come un'immagine o un video. Generalmente è meglio evitare di visualizzare i dati di questi file durante lo scaricamento, essendo meglio creare una copia locale tramite `urllib`.

\index{binary file}

Lo schema consiste nell'aprire l'URL e tramite `read` scaricare l'intero contenuto del documento in una variabile stringa (`img`) e scrivere tali informazioni in un file locale come in questo esempio:

\VerbatimInput{../code3/curl1.py}

Questo programma legge in una sola volta tutti i dati in remoto, li memorizza temporaneamente nella ram del computer nella variabile `img`, quindi apre il file `cover.jpg` e salva i dati su disco. Questa operazione funzionerà se la dimensione del file è inferiore a quella della memoria del tuo computer.

Quindi, se l'obiettivo è un file audio o video di grandi dimensioni, questo script potrebbe crashare o rallentare sensibilmente qualora il computer esaurisca la memoria. Per evitare di saturare la memoria, scarichiamo i dati in singoli blocchi (o buffer), scriviamo su disco prima di ottenere il successivo. In questo modo il programma può leggere file di qualsiasi dimensione senza utilizzare tutta la memoria ram di cui disponi nel computer.

\VerbatimInput{../code3/curl2.py}

In questo esempio stiamo leggendo solo 100.000 caratteri alla volta che poi scriviamo nel file `cover.jpg` prima di recuperare i successivi 100.000 caratteri dal web.

Questo programma funziona come puoi vedere di seguito:

~~~~
python curl2.py
568248 characters copied.
~~~~

Se hai un sistema Unix o Macintosh, probabilmente hai a disposizione un comando che può svolgere questa operazione in modo simile a quanto segue:

\index{curl}

~~~~
curl -O http://www.py4e.com/cover.jpg
~~~~

Il comando `curl` è l'abbreviazione di "copy URL" da cui ho diligentemente tratto il nome di questi due esempi `curl1.py` e `curl2.py` disponibili su [www.py4e.com/code3](http://www.py4e. com / code3) dato che implementano funzionalità simili. Esiste anche uno script `curl3.py` che esegue un po 'più efficacemente questa operazione, nel caso in cui tu desideri davvero utilizzare questo pattern in un tuo programma.

Glossario
--------

**BeautifulSoup**: libreria Python per l'analisi ed estrazione dati da documenti HTML che compensa molte delle imperfezioni nell'HTML che i browser generalmente ignorano. Puoi scaricare il codice BeautifulSoup da [www.crummy.com] (http://www.crummy.com).

\index{BeautifulSoup}

**porta**: un numero che indica in genere quale applicazione state contattando quando effettuate una connessione socket a un server. Ad esempio il traffico Web di solito utilizza la porta 80 mentre il traffico e-mail utilizza la 25.

\index{port}

**scrape**: indica il comportamento di un programma che fa finta di essere un browser web, recupera una pagina web e ne osserva il contenuto. Spesso i programmi seguono i collegamenti presenti in una pagina per trovare la pagina successiva in modo da poter scorrere all'interno di una rete di pagine o di un social network.

\index{socket}

**socket**: una connessione di rete tra due applicazioni tramite cui queste possono inviare e ricevere dati in entrambe le direzioni.

\index{socket}

**spider**: l'atto di un motore di ricerca web di recuperare una pagina e successivamente tutte le pagine collegate a questa fino a raggiungere quasi tutte le pagine in rete e utilizzarle per costruire il proprio indice di ricerca.

\index{spider}

Esercizi
---------

**Esercizio 1:** Modifica il programma socket `socket1.py` in modo da richiedere all'utente l'URL rendendolo quindi in grado di leggere qualsiasi pagina web. Puoi usare `split('/')` per suddividere l'URL nelle sue componenti in modo da poter estrarre il nome host per la chiamata `connect` del socket. Aggiungi il controllo degli errori usando `try` ed `except` per gestire la condizione in cui l'utente inserisca un URL non formattato correttamente o sia inesistente.

**Esercizio 2:** Modifica il tuo programma socket in modo che conti il numero di caratteri che ha ricevuto e interrompa la visualizzazione di qualsiasi testo dopo che ne ha mostrato 3000. Il programma dovrà inoltre accettare l'intero documento, contare il numero totale di caratteri e visualizzarne il numero.

**Esercizio 3:** Utilizza `urllib` per replicare l'esercizio precedente per (1) recuperare il documento da un URL, (2) visualizzare i primi 3000 caratteri e (3) contarne il numero complessivo. Non preoccuparti delle intestazioni per questo esercizio, per ora è sufficiente mostrare semplicemente i primi 3000 caratteri contenuti nel documento.

**Esercizio 4:** Modifica il programma `urllinks.py` per estrarre e contare i tag di paragrafo (p) dal documento HTML scaricato e visualizza il conteggio dei paragrafi come output del programma. Non visualizzare il testo del paragrafo: è sufficiente contarli. Metti alla prova il programma con diverse pagine Web e di varie dimensioni.

**Esercizio 5:** (Avanzato) Modifica il programma socket in modo che mostri i dati solo dopo che siano state ricevute le intestazioni e una riga vuota. Ricorda che `recv` sta ricevendo caratteri (inclusi caratteri newline e tutti gli altri) e non righe.
