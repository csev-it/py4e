



Come visualizzare dati
==========================

Finora abbiamo studiato il linguaggio Python per capirne l'utilizzo in internet o con i database per manipolare dati.

In questo capitolo, daremo un'occhiata a tre applicazioni che combinano tutti gli aspetti precedenti per gestire e visualizzare dati. Puoi considerare queste applicazioni come spunto per affrontare i problemi reali che potresti dover affrontare in futuro.  

Ciascuna applicazione è contenuta in un file ZIP che potrai scaricare, estrarre nel tuo computer ed eseguire.

## Costruire una Google map partendo da dati geocodificati

\index{Google!mappa}
\index{Visualizzazione!mappa}

In questo progetto, utilizziamo l'API di geocodifica di Google per gestire la posizione geografica di alcune università visualizzandole su una Google Map.

![Una Google Map](../images/google-map)

Per iniziare, scarica l'applicazione da:
[www.py4e.com/code3/geodata.zip](http://www.py4e.com/code3/geodata.zip).

Il primo problema da risolvere è dato dal fatto che le richieste gratuite di geocodifica all'API di Google sono limitate a un determinato numero. Se dovessi controllare la posizione di molte entità, potresti essere obbligato ad interrompere e riavviare il processo di ricerca diverse volte. É meglio quindi suddividere il problema in due fasi.

\index{cache}

Nella prima fase prendiamo il nostro input "survey" nel file _where.data_, ne leggiamo una riga alla volta e, dopo aver recuperato le relative informazioni geocodificate da Google, le archiviamo nel database _geodata.sqlite_. Prima di utilizzare l'API di geocodifica per ogni entità inserita dall'utente, eseguiremo un semplice controllo per verificare se siamo già in possesso dei dati relativi a quella particolare riga di input. In altre parole il database si comporta come se fosse una "cache" locale dei dati di geocodifica che ci servono in modo da evitare di chiedere a Google due o piu volte la stessa cosa.

Ti sarà possibile riavviare da zero il processo in qualsiasi momento eliminando il file _geodata.sqlite_.

Avvia ora il programma _geoload.py_. Questo script leggerà le righe di input in _where.data_ e verificherà se sono già presenti nel database. Se non disponiamo della posizione richiesta, il programma farà una chiamata all'API di Google per recuperare i dati e salvarli nel database.

Ecco un esempio di come si comporti lo script nel caso il database già contenga quanto ci serve:

    Found in database  Northeastern University
    Found in database  University of Hong Kong, ...
    Found in database  Technion
    Found in database  Viswakarma Institute, Pune, India
    Found in database  UMD
    Found in database  Tufts University

    Resolving Monash University
    Retrieving http://maps.googleapis.com/maps/api/
        geocode/json?address=Monash+University
    Retrieved 2063 characters {    "results" : [
    {'status': 'OK', 'results': ... }

    Resolving Kokshetau Institute of Economics and Management
    Retrieving http://maps.googleapis.com/maps/api/
        geocode/json?address=Kokshetau+Inst ...
    Retrieved 1749 characters {    "results" : [
    {'status': 'OK', 'results': ... }
    ...

Le prime cinque entità sono state saltate poiché erano già presenti nel database. Il programma ha continuato la verifica fino a quando ha trovato una entità con la posizione mancante e ne ha iniziato il recupero.

Il programma _geoload.py_, che può essere interrotto in qualsiasi momento, é dotato di un contatore che permette di limitare il numero di chiamate all'API di Google in ciascuna esecuzione. Dato che _where.data_ contiene solo poche centinaia di elementi di dati non dovresti superare il limite giornaliero. Nel caso tu disponga di più entità da verificare potrebbe essere necessario eseguire più volte il programma in giorni diversi prima di ottenere tutti i dati che ti servono.

Tramite lo script _geodump.py_ ti é visualizzare  i dati già caricati nel database _geodata.sqlite_. Questo programma crea il file JavaScript eseguibile _where.js_ contenente l'indirizzo, la latitudine e la longitudine di ogni entità utilizzando i dati contenuti nel tuo database.

Questo é un esempio di output del programma _geodump.py_:

    Northeastern University, ... Boston, MA 02115, USA 42.3396998 -71.08975
    Bradley University, 1501 ... Peoria, IL 61625, USA 40.6963857 -89.6160811
    ...
    Technion, Viazman 87, Kesalsaba, 32000, Israel 32.7775 35.0216667
    Monash University Clayton ... VIC 3800, Australia -37.9152113 145.134682
    Kokshetau, Kazakhstan 53.2833333 69.3833333
    ...
    12 records written to where.js
    Open where.html to view the data in a browser

Il file _where.html_, costituito da codice HTML e JavaScript, permette di visualizzare in una Google Map i dati più recenti contenuti in _where.js_.
Questo é un esempio della struttura del file _where.js_:

```{.js}
myData = [
[42.3396998,-71.08975, 'Northeastern Uni ... Boston, MA 02115'],
[40.6963857,-89.6160811, 'Bradley University, ... Peoria,
IL 61625, USA'], [32.7775,35.0216667, 'Technion, Viazman 87,
Kesalsaba, 32000, Israel'],
   ...
];
```

Questa è una variabile JavaScript che contiene un elenco di elenchi. La sintassi per le costanti dell'elenco JavaScript dovrebbe esserti familiare in quanto molto simile a quella di Python.

Per poter vedere le posizioni geografiche delle entità devi aprire _where.html_ in un browser. Passando con il mouse su ciascun marker della mappa vedrai la posizione che restituita dall'API di geocodifica. Se aprendo il file _where.html_ non riesci a vedere alcun dato, prova a verificare le impostazioni della console JavaScript o sviluppatore del tuo browser.

## Visualizzare reti e interconnessioni

\index{Google!page rank}
\index{Visualizzazione!reti}
\index{Visualizzazione!rank della pagina}

Tramite questa applicazione sperimenteremo alcune funzioni tipiche di un motore di ricerca. Per prima cosa analizzeremo una piccola porzione del web e, tramite una versione semplificata dell'algoritmo di ranking delle pagine di Google, determineremo quali pagine siano maggiormente connesse. Infine visualizzeremo il rank delle singole pagine e la connessioni del nostro piccolo angolo di Rete. Utilizzeremo la libreria JavaScript D3 <http://d3js.org/> per rappresentare visivamente l'output.

Puoi scaricare ed estrarre questa applicazione da: [www.py4e.com/code3/pagerank.zip](http://www.py4e.com/code3/pagerank.zip)

![A Page Ranking](height=3.5in@../images/pagerank)

Il primo script (_spider.py_) esegue la scansione di un sito Web e inserisce la serie di pagine nel database (_spider.sqlite_), registrando anche i collegamenti tra le singole pagine. È possibile riavviare da zero il processo in qualsiasi momento eliminando il file _spider.sqlite_ ed eseguendo nuovamente _spider.py_.

    Enter web url or enter: http://www.dr-chuck.com/
    ['http://www.dr-chuck.com']
    How many pages:2
    1 http://www.dr-chuck.com/ 12
    2 http://www.dr-chuck.com/csev-blog/ 57
    How many pages:

In questo esempio, abbiamo indicato al programma di eseguire la scansione di un sito Web specifico (<http://www.dr-chuck.com/>) e di recuperarne due pagine. Se dovessi riavviare lo script per controllare altre pagine, ricorda che quelle già presenti nel database non verranno prese in considerazione; lo spider invece avvierà il processo di acquisizione partendo da una pagina scelta in modo casuale tra quelle non ancora acquisite. Ogni successiva esecuzione di _spider.py_ è quindi da considerare incrementale rispetto alle precedenti.

    Enter web url or enter: http://www.dr-chuck.com/
    ['http://www.dr-chuck.com']
    How many pages:3
    3 http://www.dr-chuck.com/csev-blog 57
    4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1
    5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13
    How many pages:

in altre parole nello stesso database/programma possono essere presenti più punti di partenza potenziali, chiamati "web". Lo spider sceglie casualmente la successiva pagina da elaborare tra tutti i link non ancora visitati.

Tramite _spdump.py_ puoi scaricare il contenuto del file _spider.sqlite_:

    (5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')
    (3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
    (1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')
    (1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
    4 rows.

Questo comando ti permette di visualizzare il numero di collegamenti in entrata, la precedente e la nuova classificazione della pagina, il suo identificativo ed il  suo indirizzo. Nota che _spdump.py_ mostra solo le pagine che contengono almeno un collegamento in entrata.

Una volta che avrai popolato il database con un numero sufficiente di pagine, puoi calcolare il rank delle pagine tramite _sprank.py_. Devi semplicemente indicargli quante iterazioni di classificazione delle pagine eseguire.

    How many iterations:2
    1 0.546848992536
    2 0.226714939664
    [(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]

Puoi scaricare nuovamente il contenuto del database per vedere se la classificazione delle pagine è stata modificata:

    (5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')
    (3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
    (1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')
    (1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
    4 rows.

Puoi eseguire _sprank.py_ a volontà: ad ogni esecuzione verrà raffinata la classificazione delle pagine. Puoi anche eseguire _sprank.py_ un paio di volte, acquisire altre pagine con _spider.py_ e quindi eseguire nuovamente _sprank.py_ per ricontrollare il rank delle pagine. Generalmente un motore di ricerca esegue  contemporaneamente la scansione ed il rank delle pagine.

Se desideri, avviando prima _spreset.py_ e successivamente _sprank.py_, puoi resettare il rank delle pagine senza effettuare nuovamente lo "spidering" delle pagine Web.

    How many iterations:50
    1 0.546848992536
    2 0.226714939664
    3 0.0659516187242
    4 0.0244199333
    5 0.0102096489546
    6 0.00610244329379
    ...
    42 0.000109076928206
    43 9.91987599002e-05
    44 9.02151706798e-05
    45 8.20451504471e-05
    46 7.46150183837e-05
    47 6.7857770908e-05
    48 6.17124694224e-05
    49 5.61236959327e-05
    50 5.10410499467e-05
    [(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]

Ad ogni iterazione di questo algoritmo viene visualizzata la variazione media del rank della pagina. Dato che all'inizio la rete è piuttosto sbilanciata, potrai notare una notevole variazione iniziale del rank delle pagine che, nel corso delle varie iterazioni tenderà successivamente a stabilizzarsi. È il caso che tu esegua _sprank.py_ fino a quando noti che questo valore non subisce variazioni significative.

Grazie a _spjson.py_ potrai visualizzare le pagine attualmente con rank maggiore: i loro valori verranno estratti dal database e salvati in un formato JSON.

    Creating JSON output on spider.json...
    How many nodes? 30
    Open force.html in a browser to view the visualization

Potrai visualizzare la struttura dei nodi e dei collegamenti relativi a questi dati aprendo il file _force.html_ in un browser. una volta aperto il file ti sarà possibile, tramite un clic, trascinare qualsiasi nodo e, tramite il doppio clic, visualizzare l'URL di un nodo specifico.

Se nel frattempo utilizzi una delle funzioni precedenti, potrai consultare i dati aggiornati contenuti in _spider.json_ rieseguendo nuovamente _spjson.py_ e premendo F5 una volta ritornato sul browser.

## Visualizzazione dei dati della posta elettronica

Arrivato a questo punto del libro, dovresti aver acquisito una discreta familiarità con i file _mbox-short.txt_ e _mbox.txt_. Ora è il momento di portare ad un livello superiore la tua capacità di analisi dei dati di posta elettronica.

Nel mondo reale potrebbe capitarti di dover estrarre i dati di email dai server di posta elettronica. Questa operazione può richiedere tempo e i dati ottenuti potrebbero essere incoerenti, pieni di errori al punto da richiedere attività di normalizzazione o rettifica.
In questa sezione, lavoreremo con l'applicazione più complessa tra quelle finora esaminate che ti permetterà di acquisire e visualizzare quasi un gigabyte di dati.

![A Word Cloud from the Sakai Developer List](height=3.5in@../images/wordcloud)

È possibile scaricare questa applicazione da:

[www.py4e.com/code3/gmane.zip](http://www.py4e.com/code3/gmane.zip).

Utilizzeremo i dati di un servizio gratuito di archiviazione di elenchi di posta elettronica chiamato [www.gmane.org](http://www.gmane.org). Questo servizio è molto popolare fra i progetti open source sia perché fornisce un archivio consultabile della loro attività di posta elettronica sia perché ha anche una politica molto liberale sull'accesso ai dati attraverso la loro API. Non hanno limiti di velocità nella consultazione, ma richiedono di non sovraccaricare il loro servizio e di prelevare solo i dati necessari. Potrai leggere i termini e le condizioni di gmane in questa pagina:

<http://gmane.org/export.php>.

_È molto importante usare responsabilmente i dati di gmane.org inserendo delle pause durante l'accesso ai loro servizi e distribuendo i compiti più prolungati in un arco di tempo più lungo. Non abusare di questo servizio gratuito e non rovinarlo a discapito di tutti noi._

L'estrazione dei dati delle e-mail di Sakai tramite il nostro spider richiederà diversi giorni ed alla fine essi occuperanno quasi un gigabyte di spazio. Il file _README.txt_ contenuto nello ZIP sopra riportato dovrebbe contenere istruzioni su come scaricare una copia parziale del file _content.sqlite_ contenente la maggior parte dell'insieme delle e-mail di Sakai che ti eviterà di dover passare quasi una settimana a scaricare dati. anche se scarichi questo file, dovrai comunque far girare lo spider per recuperare i messaggi più recenti.  

Il primo passo è quello di lanciare lo spider sul repository gmane. L'URL di base è inserito direttamente in _gmane.py_ ed è presente nell'elenco degli sviluppatori di Sakai. Puoi esaminare in qualunque momento un altro repository cambiando l'url di base ed assicurandoti di eliminare il file _content.sqlite_.  

Il file _gmane.py_ opera come uno spider responsabile dato che la sua velocità di esecuzione é lenta: recupera un messaggio di posta elettronica al secondo in modo da evitare di essere limitato da gmane. Memorizza tutti i  dati in un database e può essere interrotto e riavviato tutte le volte che sia necessario. Dato che probabilmente saranno necessarie molte ore per scaricare tutti i dati, potrebbe  essere necessario riavviare più volte questo script più volte.

Ecco un esempio dell'esecuzione di _gmane.py_ durante il recupero degli ultimi cinque messaggi dalla lista degli sviluppatori di Sakai:

    How many messages:10
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
        nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
        samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
        da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
        m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
        samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
    http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

    Does not start with From

Il programma esegue un controllo di _content.sqlite_ partendo dal primo messaggio fino a quello non ancora esaminato ed avvia la ricerca automatica partendo da quello poi continua a lavorare fino a quando non ha esaminato il numero desiderato di messaggi o raggiunge una pagina che non somiglia a un messaggio formattato correttamente.  

A volte capita che in [gmane.org](gmane.org) manchi un messaggio. Forse gli amministratori possono eliminare messaggi o forse si perdono. Se lo spider si ferma e sembra che abbia raggiunto una posizione con un messaggio mancante, tramite l'SQLite Manager aggiungi una riga con l'id mancante lasciando vuoti tutti gli altri campi e riavvia _gmane.py_. In questo modo potrai sbloccare il processo di spidering e permetterà al sistema di continuare. Questi messaggi vuoti verranno ignorati nella fase successiva dell'analisi.  

Il bello è che una volta che hai esaminato automaticamente tutti i messaggi e li hai inseriti in _content.sqlite_, puoi eseguire nuovamente _gmane.py_ per ottenere i messaggi che man mano vengono inviati alla lista.

I dati contenuti in _content.sqlite_ sono piuttosto grezzi, non compressi e con una struttura inefficiente. Questo è intenzionale in quanto ti consente di esaminare _content.sqlite_ tramite SQLite Manager per eseguire il debug dei problemi derivanti dal processo di spidering.  Eseguire delle query su questo database è probabilmente una cattiva idea poiché queste sarebbero piuttosto lente.  

La seconda parte del processo di analisi viene eseguita dallo script _gmodel.py_: i dati grezzi contenuti in  _content.sqlite_ vengono letti ed analizzati e viene ne prodotto una versione pulita e ben modellata nel file _index.sqlite_. Questo database é normalmente molto più piccolo rispetto a _content.sqlite_ (spesso fino a 10 volte minore rispetto all'originario) perché viene compresso il testo dell'intestazione del corpo.  

Ogni volta che esegui _gmodel.py_, lo script elimina e ricostruisce _index.sqlite_, permettendoti di regolare i suoi parametri e modificare le tabelle di mappatura in _content.sqlite_ per ottimizzare il processo di pulizia dei dati. Qui sotto è presente un esempio della esecuzione di _gmodel.py_: viene Visualizzata una riga ogni volta che vengono elaborati 250 messaggi di posta elettronica in modo da avvisarti dei progressi dell'analisi dato che questa potrebbe protrarsi per un bel po' prima di terminare l'elaborazione di quasi un gigabyte di dati di posta.

    Loaded allsenders 1588 and mapping 28 dns mapping 1
    1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
    251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
    501 2006-01-12T11:17:34-05:00 lance@indiana.edu
    751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
    ...

Il programma _gmodel.py_ gestisce una serie di attività di pulizia dei dati.

I nomi di dominio sono troncati a due livelli per i .com, .org, .edu e .net. Altri nomi di dominio sono troncati a tre livelli. Quindi si.umich.edu diventa umich.edu e caret.cam.ac.uk diventa cam.ac.uk. Gli indirizzi di posta elettronica sono anche riscritti in lettere minuscole ed alcuni degli indirizzi di @gmane.org simili a:

    arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org

vengono convertiti nell'indirizzo reale ogni volta che c'è una corrispondenza con un indirizzo e-mail reale corrispondente in un punto qualsiasi del corpo del messaggio.

Nel database _mapping.sqlite_ sono presenti due tabelle che ti consentono di mappare sia i nomi di dominio sia i singoli indirizzi e-mail che cambiano nel corso della vita della mailing list. Ad esempio, Steve Githens ha utilizzato i seguenti indirizzi email quando ha cambiato lavoro durante la vita della lista degli sviluppatori di Sakai:

    s-githens@northwestern.edu
    sgithens@cam.ac.uk
    swgithen@mtu.edu

Possiamo aggiungere due voci alla tabella Mapping in _content.sqlite_ per fare sì che _gmodel.py_ mappi correttamente tutti e tre gli indirizzi:

    s-githens@northwestern.edu ->  swgithen@mtu.edu
    sgithens@cam.ac.uk -> swgithen@mtu.edu

ti è inoltre possibile creare voci simili nella tabella DNSMapping se ci sono più nomi DNS che desideri associare a un singolo DNS. La seguente mappatura è stata aggiunta ai dati di Sakai:

    iupui.edu -> indiana.edu

in modo che tutti gli account dei vari campus della Indiana University siano tracciati insieme.

ti è possibile eseguire _gmodel.py_ ripetutamente mentre esamini i dati ed aggiungere tutte le mappature che ritieni necessarie per avere i dati sempre ordinati. Al termine d di questa fase, avrai ottenuto una versione ben indicizzata delle email in _index.sqlite_. Questo è il file che verrà  utilizzato per eseguire l'analisi vera e propria dei dati che sarà molto rapida.

La prima semplice analisi dei dati è determinare "chi ha inviato la maggior parte dei messaggi" e "quale organizzazione ha inviato la maggior parte della email".
Ciò é possibile tramite lo script _gbasic.py_:

    How many to dump? 5
    Loaded messages= 51330 subjects= 25033 senders= 1584

    Top 5 Email list participants
    steve.swinsburg@gmail.com 2657
    azeckoski@unicon.net 1742
    ieb@tfd.co.uk 1591
    csev@umich.edu 1304
    david.horwitz@uct.ac.za 1184

    Top 5 Email list organizations
    gmail.com 7339
    umich.edu 6243
    uct.ac.za 2451
    indiana.edu 2258
    unicon.net 2055

Nota che quanto rapidamente viene eseguito _gbasic.py_  rispetto a _gmane.py_ o anche _gmodel.py_. tutti gli script lavorando sugli stessi dati, ma _gbasic.py_ sta usando i dati compressi e normalizzati in _index.sqlite_. Se devi gestire molti dati, un processo a più fasi come quello di questa applicazione potrebbe richiedere un po' più di tempo per essere sviluppato ma ti farà risparmiare un sacco di tempo quando inizierai veramente a esplorare e visualizzare i tuoi dati.

Puoi ottenere una prima visualizzazione della frequenza delle parole dell'oggetto delle email tramite _gword.py_:

    Range of counts: 33229 129
    Output written to gword.js

Il risultato viene salvato nel file _gword.js_ che puoi visualizzare tramite _gword.htm_ per produrre una nuvola di parole simile a quella mostrata all'inizio di questa sezione.

Una seconda visualizzazione è prodotta da _gline.py_ in cui viene calcolata la partecipazione via e-mail da parte delle organizzazioni nel corso del tempo.

    Loaded messages= 51330 subjects= 25033 senders= 1584
    Top 10 Oranizations
    ['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
    'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
    'stanford.edu', 'ox.ac.uk']
    Output written to gline.js

L'output viene salvato in _gline.js_ che può essere visualizzato tramite _gline.htm_.

![Sakai Mail Activity by Organization](../images/mailorg)

Questa applicazione, relativamente complessa e sofisticata,  ti ha permesso di esplorare le funzionalità di recupero, normalizzati e visualizzazione di dati reali.
