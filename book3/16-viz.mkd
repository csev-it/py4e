
%

Visualizzare i dati
========================

Finora abbiamo studiato il linguaggio Python per capirne le logiche interne, le funzioni che offre per la interagire con la rete o i database per manipolare dati.

In questo capitolo, studieremo tre applicazioni dotate delle capacità di gestire e visualizzare i dati. Considerarle degli spunti da cui partire e risolvere problemi reali.  

Ciascuna applicazione è contenuta in un file ZIP che potrai scaricare, estrarre sul tuo computer ed eseguire.

Costruire un Google map a partire da dati geocodificati
----------------------------------------

\index{Google!map}
\index{Visualization!map}

In questo progetto, utilizziamo l'API di geocodifica di Google per riordinare alcune posizioni geografiche di nomi di università immesse dall'utente e inserire quindi i dati su una mappa di Google.

![A Google Map](../images/google-map)

Per iniziare, scaricate l'applicazione da: [www.py4e.com/code3/geodata.zip](http://www.py4e.com/code3/geodata.zip). Il primo problema da risolvere è dato dal fatto che la geocodifica gratuita dell'API di Google è limitata a un determinato numero di richieste al giorno. Se si dispone di molti dati, potrebbe essere necessario interrompere e riavviare il processo di ricerca diverse volte. Suddividiamo pertanto il problema in due fasi.

\index{cache}

Nella prima fase prendiamo il nostro input "survey" nel file *where.data* e lo leggiamo una riga alla volta, recuperiamo le informazioni geocodificate da Google e le archiviamo in un database *geodata.sqlite*. Prima di utilizzare l'API di geocodifica per ogni posizione inserita dall'utente, eseguiamo un semplice controllo per vedere se abbiamo già dati per quella particolare riga di input. Il database funziona come una "cache" locale dei nostri dati di geocodifica per non chiedere mai a Google gli stessi dati due volte.  È possibile riavviare il processo in qualsiasi momento cancellando il file *geodata.sqlite*.  Eseguite il programma *geoload.py*. Questo programma leggerà le righe di input in *where.data* e per ogni riga controllerà se è già presente nel database. Se non disponiamo dei dati della posizione, il programma chiamerà l'API di geocodifica per recuperare i dati e archiviarli nel database.  Ecco un esempio di esecuzione dopo che sono già stati inseriti alcuni dati nel database:

~~~~
Found in database  Northeastern University
Found in database  University of Hong Kong, ...
Found in database  Technion
Found in database  Viswakarma Institute, Pune, India
Found in database  UMD
Found in database  Tufts University

Resolving Monash University
Retrieving http://maps.googleapis.com/maps/api/
    geocode/json?address=Monash+University
Retrieved 2063 characters {    "results" : [
{'status': 'OK', 'results': ... }

Resolving Kokshetau Institute of Economics and Management
Retrieving http://maps.googleapis.com/maps/api/
    geocode/json?address=Kokshetau+Inst ...
Retrieved 1749 characters {    "results" : [
{'status': 'OK', 'results': ... }
...
~~~~

Le prime cinque posizioni sono già nel database e quindi vengono saltate. Il programma esegue la scansione fino al punto in cui trova nuove posizioni e inizia a recuperarle.  Il programma *geoload.py* può essere interrotto in qualsiasi momento e esiste un contatore che è possibile utilizzare per limitare il numero di chiamate all'API di geocodifica per ciascuna esecuzione. Dato che *where.data* ha solo poche centinaia di elementi di dati, non dovreste esaurire il limite giornaliero, ma se disponete di più dati potrebbero essere necessarie più esecuzioni per diversi giorni per far sì che il vostro database contenga tutti i dati di geocodificazione relativi al vostro input.  Dopo aver caricato alcuni dati in *geodata.sqlite*, è possibile visualizzarli utilizzando il programma *geodump.py*. Questo programma legge il database e scrive il file *where.js* contenente la posizione, la latitudine e la longitudine sotto forma di codice JavaScript eseguibile.  Una esecuzione del programma *geodump.py* fornisce i seguenti dati:

~~~~
Northeastern University, ... Boston, MA 02115, USA 42.3396998 -71.08975
Bradley University, 1501 ... Peoria, IL 61625, USA 40.6963857 -89.6160811
...
Technion, Viazman 87, Kesalsaba, 32000, Israel 32.7775 35.0216667
Monash University Clayton ... VIC 3800, Australia -37.9152113 145.134682
Kokshetau, Kazakhstan 53.2833333 69.3833333
...
12 records written to where.js
Open where.html to view the data in a browser
~~~~

Il file *where.html* è costituito da linguaggi HTML e JavaScript per visualizzare una mappa di Google. Legge i dati più recenti in *where.js* per visualizzare i dati. Ecco il formato del file *where.js*:

~~~~ {.js}
myData = [
[42.3396998,-71.08975, 'Northeastern Uni ... Boston, MA 02115'],
[40.6963857,-89.6160811, 'Bradley University, ... Peoria, IL 61625, USA'],
[32.7775,35.0216667, 'Technion, Viazman 87, Kesalsaba, 32000, Israel'],
   ...
];
~~~~

Questa è una variabile JavaScript che contiene un elenco di elenchi. La sintassi per le costanti dell'elenco JavaScript è molto simile a quella di Python, quindi dovrebbe esservi familiare.  Aprite semplicemente *where.html* in un browser per vedere le posizioni. Potete passare con il mouse su ciascun simbolo della mappa per trovare la posizione che l'API di geocodifica ha restituito per l'input inserito dall'utente. Se non riuscite a vedere alcun dato quando aprite il file *where.html*, potrebbe essere necessario controllare il codice JavaScript o la console di sviluppo del vostro browser.

Visualizzazione di reti e interconnessioni
-----------------------------------------

\index{Google!page rank}
\index{Visualization!networks}
\index{Visualization!page rank}

In questa applicazione eseguiremo alcune delle funzioni di un motore di ricerca. Per prima cosa analizzeremo un piccolo sottoinsieme del web ed eseguiremo una versione semplificata dell'algoritmo della classificazione delle pagine di Google per determinare quali pagine sono maggiormente connesse e quindi visualizzare la classifica della pagina e la connettività del nostro piccolo angolo di web. Utilizzeremo la libreria JavaScript D3 <http://d3js.org/> per produrre l'output da visualizzare.  Potete scaricare ed estrarre questa applicazione da: [www.py4e.com/code3/pagerank.zip](http://www.py4e.com/code3/pagerank.zip)

![A Page Ranking](height=3.5in@../images/pagerank)

Il primo programma (*spider.py*) esegue la scansione di un sito Web e inserisce una serie di pagine nel database (*spider.sqlite*), registrando i collegamenti tra le pagine. È possibile riavviare il processo in qualsiasi momento cancellando il file *spider.sqlite* e rieseguendo *spider.py*.

~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:2
1 http://www.dr-chuck.com/ 12
2 http://www.dr-chuck.com/csev-blog/ 57
How many pages:
~~~~

In questa sessione di esempio, abbiamo richiesto di eseguire la scansione di un sito Web e recuperare due pagine. Se si riavvia il programma e gli si dice di eseguire la scansione di più pagine, non eseguirà nuovamente la scansione delle pagine già presenti nel database. Al riavvio passa a una pagina casuale non scansionata e inizia da lì. Ogni successiva esecuzione di *spider.py* è quindi additiva.

~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:3
3 http://www.dr-chuck.com/csev-blog 57
4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1
5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13
How many pages:
~~~~

Possiamo avere più punti di partenza nello stesso database, all'interno del programma, sono chiamati "griglie". Lo spider sceglie casualmente tra tutti i link non visitati nel web la successiva pagina da elaborare.  Per scaricare il contenuto del file *spider.sqlite*, potete eseguire *spdump.py* nel seguente modo:

~~~~
(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')
(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

Viene visualizzato il numero di collegamenti in entrata, la precedente classificazione della pagina, quella nuova, l'identificativo e l'indirizzo della pagina. Il programma *spdump.py* mostra solo le pagine che contengono almeno un collegamento in entrata.  Una volta che alcune pagine hanno popolato il database, potete eseguire ila classificazione delle pagine usando il programma *sprank.py*. Dovete semplicemente indicare quante iterazioni di classificazione di pagine eseguire.

~~~~
How many iterations:2
1 0.546848992536
2 0.226714939664
[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]
~~~~

Potete scaricare nuovamente il database per vedere che la classificazione delle pagine è stata aggiornata:

~~~~
(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')
(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

Potete eseguire *sprank.py* a volontà: ogni volta che verrà eseguito, verrà semplicemente raffinata la classificazione delle pagine. Potete anche eseguire *sprank.py* un paio di volte e poi fare uno spidering di alcune pagine con *spider.py* e quindi eseguire *sprank.py* per ricontrollare i valori della classificazione delle pagine. Generalmente un motore di ricerca esegue sia la scansione sia i programmi di classificazione contemporaneamente.  Se si desidera riavviare i calcoli della classificazione della pagina senza effettuare nuovamente lo spidering delle pagine Web, è possibile utilizzare *spreset.py* e quindi riavviare *sprank.py*.

~~~~
How many iterations:50
1 0.546848992536
2 0.226714939664
3 0.0659516187242
4 0.0244199333
5 0.0102096489546
6 0.00610244329379
...
42 0.000109076928206
43 9.91987599002e-05
44 9.02151706798e-05
45 8.20451504471e-05
46 7.46150183837e-05
47 6.7857770908e-05
48 6.17124694224e-05
49 5.61236959327e-05
50 5.10410499467e-05
[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]
~~~~

Per ogni iterazione dell'algoritmo di classificazione delle pagine, viene visualizzata la variazione media della classificazione per pagina. La rete inizialmente è abbastanza sbilanciata e quindi i singoli valori di classificazione delle pagine cambiano moltissimo nel corso delle iterazioni. Ma in poche brevi iterazioni, la classificazione delle pagine si stabilizza. Dovreste eseguire *sprank.py* abbastanza a lungo da far stabilizzare i valori della classificazione delle pagine.  Se desiderate visualizzare le pagine attualmente in cima alla classifica, eseguite *spjson.py* per leggere il database e scrivere i dati delle pagine maggiormente collegate in formato JSON, in modo da visualizzarli in un browser.

~~~~
Creating JSON output on spider.json...
How many nodes? 30
Open force.html in a browser to view the visualization
~~~~

Potete visualizzare questi dati aprendo il file *force.html* nel vostro browser. Questo visualizza automaticamente la struttura dei nodi e dei collegamenti. Potete fare clic e trascinare qualsiasi nodo e potete anche fare doppio clic su un nodo per trovare l'URL rappresentato dal nodo.  Se si eseguono nuovamente le altre utilità, rieseguite *spjson.py* e aggiornate il browser per ottenere i nuovi dati da *spider.json*.

Visualizzazione dei dati della posta elettronica
---------------------

Fino a questo punto del libro, avete acquisito una buona familiarità con i nostri file di dati *mbox-short.txt* e *mbox.txt*. Ora è il momento di portare la nostra analisi dei dati di posta elettronica al livello successivo.  Nel mondo reale, a volte è necessario estrarre i dati della posta dai server. Ciò potrebbe richiedere un po' di tempo e i dati potrebbero essere incoerenti, pieni di errori e potrebbe essere necessario un riordino e molte rettifiche. In questa sezione, lavoreremo con un'applicazione che è la più complessa di quelle finora esaminate e acquisiremo e visualizzeremo quasi un gigabyte di dati.

![A Word Cloud from the Sakai Developer List](height=3.5in@../images/wordcloud)

È possibile scaricare questa applicazione da:

[www.py4e.com/code3/gmane.zip](http://www.py4e.com/code3/gmane.zip).

Utilizzeremo i dati di un servizio gratuito di archiviazione di elenchi di posta elettronica chiamato [www.gmane.org] (http://www.gmane.org). Questo servizio è molto popolare fra i progetti open source perché fornisce un archivio consultabile della propria attività di posta elettronica. Hanno anche una politica molto liberale per quanto riguarda l'accesso ai loro dati attraverso la loro API. Non hanno limiti di velocità, ma richiedono di non sovraccaricare il loro servizio e di prelevare solo i dati necessari. Potete leggere i termini e le condizioni di gmane in questa pagina:

<http://gmane.org/export.php>.

*È molto importante usare responsabilmente i dati di gmane.org aggiungendo delle pause all'accesso ai loro servizi e distribuendo i compiti di lunga durata per un periodo di tempo più lungo. Non abusate di questo servizio gratuito e non rovinatelo a discapito di tutti noi.*

Quando i dati delle e-mail di Sakai sono stati esaminati con lo spider utilizzando questo software, hanno prodotto quasi un gigabyte di dati e hanno richiesto un numero di analisi che sono durate diversi giorni. Il file *README.txt* nello ZIP sopra riportato può avere istruzioni su come scaricare una copia di pre-esame del file *content.sqlite* per la maggior parte dell'insieme delle e-mail di Sakai in modo da non dover far eseguire lo spider per cinque giorni solo per eseguire i programmi. Se scaricate il contenuto di pre-esame, dovreste comunque eseguire il processo di spidering per recuperare i messaggi più recenti.  

Il primo passo è quello di lanciare lo spider sul repository gmane. L'URL di base è inserito direttamente in *gmane.py* ed è inserito nell'elenco degli sviluppatori di Sakai. Potete esaminare un altro repository cambiando l'url di base. Assicuratevi di cancellare il file *content.sqlite* se cambiate l'URL di base.  

Il file *gmane.py* opera come uno spider di cache responsabile in quanto viene eseguito lentamente e recupera un messaggio di posta elettronica al secondo in modo da evitare di essere limitato da gmane. Memorizza tutti i suoi dati in un database e può essere interrotto e riavviato tutte le volte che è necessario. Potrebbero essere necessarie molte ore per scaricare tutti i dati, potrebbe quindi essere necessario riavviare più volte.  Ecco una serie di *gmane.py* che recupera gli ultimi cinque messaggi dell'elenco degli sviluppatori di Sakai:

~~~~
How many messages:10
http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

Does not start with From
~~~~


Il programma esegue la scansione di *content.sqlite* da uno fino al primo numero di messaggio non già esaminato e inizia la ricerca automatica da quel messaggio. Continua a lavorare fino a quando non ha esaminato il numero desiderato di messaggi o raggiunge una pagina che non somiglia a un messaggio formattato correttamente.  

A volte in [gmane.org] (gmane.org) manca un messaggio. Forse gli amministratori possono cancellare messaggi o forse si perdono. Se il vostro spider si ferma e sembra che abbia raggiunto una posizione con un messaggio mancante, andate in SQLite Manager e aggiungete una riga con l'id mancante lasciando vuoti tutti gli altri campi e riavviate *gmane.py*. Questo sbloccherà il processo di spidering e gli permetterà di continuare. Questi messaggi vuoti verranno ignorati nella fase successiva del processo.  

Il bello è che una volta che avete esaminato automaticamente tutti i messaggi e li avete inseriti in *content.sqlite*, potete eseguire *gmane.py* di nuovo per ottenere nuovi messaggi man mano che vengono inviati all'elenco.  I dati *content.sqlite* sono piuttosto grezzi, con una struttura inefficiente e non sono compressi. Questo è intenzionale in quanto consente di esaminare *content.sqlite* in SQLite Manager per eseguire il debug dei problemi con il processo di spidering. Sarebbe una cattiva idea eseguire query su questo database, poiché sarebbero piuttosto lente.  

Il secondo processo consiste nell'eseguire il programma *gmodel.py*. Questo programma legge i dati grezzi da *content.sqlite* e produce una versione pulita e ben modellata dei dati nel file *index.sqlite*. Questo file sarà molto più piccolo (spesso 10 volte più piccolo) rispetto a *content.sqlite* perché comprime anche l'intestazione e il corpo.  

Ogni volta che *gmodel.py* esegue, cancella e ricostruisce *index.sqlite*, vi consente di regolare i suoi parametri e modificare le tabelle di mappatura in *content.sqlite* per ottimizzare il processo di pulizia dei dati. Questa è un'esecuzione di esempio di *gmodel.py*. Visualizza una riga ogni volta che vengono elaborati 250 messaggi di posta elettronica in modo da poter vedere alcuni progressi, poiché questo programma potrebbe restare in esecuzione per un bel po' per elaborare quasi un gigabyte di dati di posta.

~~~~
Loaded allsenders 1588 and mapping 28 dns mapping 1
1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
501 2006-01-12T11:17:34-05:00 lance@indiana.edu
751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
...
~~~~

Il programma *gmodel.py* gestisce una serie di attività di pulizia dei dati.  I nomi di dominio sono troncati su due livelli per .com, .org, .edu e .net. Altri nomi di dominio sono troncati su tre livelli. Quindi si.umich.edu diventa umich.edu e caret.cam.ac.uk diventa cam.ac.uk. Gli indirizzi di posta elettronica sono anche trasformati in lettere minuscole e alcuni degli indirizzi di @gmane.org come questi:

~~~~
arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org
~~~~

vengono convertiti nell'indirizzo reale ogni volta che c'è un indirizzo e-mail reale corrispondente in un'altra parte del corpo del messaggio.  Nel database *content.sqlite* ci sono due tabelle che consentono di mappare sia i nomi di dominio che i singoli indirizzi e-mail che cambiano nel corso della vita della mailing list. Ad esempio, Steve Githens ha utilizzato i seguenti indirizzi email mentre ha cambiato lavoro durante la vita dell'elenco degli sviluppatori di Sakai:

~~~~
s-githens@northwestern.edu
sgithens@cam.ac.uk
swgithen@mtu.edu
~~~~

Possiamo aggiungere due voci alla tabella Mapping in *content.sqlite* così *gmodel.py* mapperà tutti e tre gli indirizzi:

~~~~
s-githens@northwestern.edu ->  swgithen@mtu.edu
sgithens@cam.ac.uk -> swgithen@mtu.edu
~~~~

È inoltre possibile creare voci simili nella tabella DNSMapping se vi sono più nomi DNS che si desidera associare a un singolo DNS. La seguente mappatura è stata aggiunta ai dati di Sakai:

~~~~
iupui.edu -> indiana.edu
~~~~

così tutti gli account dei vari campus della Indiana University sono tracciati insieme.  È possibile eseguire *gmodel.py* ripetutamente mentre si esaminano i dati e aggiungere mappature per avere i dati sempre più in ordine. Al termine, avrete una versione ben indicizzata delle email in *index.sqlite*. Questo è il file da utilizzare per eseguire l'analisi dei dati. Con questo file, l'analisi dei dati sarà molto veloce.  La prima semplice analisi dei dati è determinare "chi ha inviato la maggior parte della posta?" e "quale organizzazione ha inviato la maggior parte della posta"? Questo viene fatto usando *gbasic.py*:

~~~~
How many to dump? 5
Loaded messages= 51330 subjects= 25033 senders= 1584

Top 5 Email list participants
steve.swinsburg@gmail.com 2657
azeckoski@unicon.net 1742
ieb@tfd.co.uk 1591
csev@umich.edu 1304
david.horwitz@uct.ac.za 1184

Top 5 Email list organizations
gmail.com 7339
umich.edu 6243
uct.ac.za 2451
indiana.edu 2258
unicon.net 2055
~~~~

Notate quanto più rapidamente *gbasic.py* viene eseguito rispetto a *gmane.py* o anche *gmodel.py*. Stanno tutti lavorando sugli stessi dati, ma *gbasic.py* sta usando i dati compressi e normalizzati in *index.sqlite*. Se dovete gestire molti dati, un processo a più fasi come quello di questa applicazione potrebbe richiedere un po' più di tempo per svilupparsi, ma vi farà risparmiare un sacco di tempo quando iniziate veramente a esplorare e visualizzare i vostri dati.  Potete produrre una semplice visualizzazione della frequenza delle parole nelle righe dell'oggetto nel file *gword.py*:

~~~~
Range of counts: 33229 129
Output written to gword.js
~~~~

Questo produce il file *gword.js* che potete visualizzare usando *gword.htm* per produrre una nuvola di parole simile a quella mostrata all'inizio di questa sezione.  Una seconda visualizzazione è prodotta da *gline.py*. Calcola la partecipazione alle e-mail da parte delle organizzazioni nel tempo.

~~~~
Loaded messages= 51330 subjects= 25033 senders= 1584
Top 10 Oranizations
['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
'stanford.edu', 'ox.ac.uk']
Output written to gline.js
~~~~

Il suo output è scritto in *gline.js* che viene visualizzato usando *gline.htm*.

![Sakai Mail Activity by Organization](../images/mailorg)

Questa è un'applicazione relativamente complessa e sofisticata e dispone di funzionalità per recuperare, ordinare e visualizzare dati reali.
